{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to create the dataset will be:\n",
    "\n",
    "1. Fetch any pdf\n",
    "2. Select a random page from that pdf\n",
    "3. Extract all the facts for that page\n",
    "4. Create flashcards for each of the facts extracted\n",
    "5. Output the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the output from the prompt engineering experiments to a JSON file\n",
    "def save_output(text_input, prompt_candidates, output, model, approach):\n",
    "    data = {\n",
    "        \"text_input\": text_input,\n",
    "        \"prompt_candidates\": prompt_candidates,\n",
    "        \"output\": output,\n",
    "        \"model\": model,\n",
    "        \"approach\": approach\n",
    "    }\n",
    "\n",
    "    with open('output.json', 'a') as file:\n",
    "        json.dump(data, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2. Requirements For This Book 4\n",
      "1.2 Requirements For This Book\n",
      "1.2.1 Python and SciPy\n",
      "You do not need to be a Python expert, but it would be helpful if you knew how to install and\n",
      "setup Python and SciPy. The tutorials assume that you have a Python and SciPy environment\n",
      "available. This may be on your workstation or laptop, it may be in a VM or a Docker instance\n",
      "that you run, or it may be a server instance that you can con\fgure in the cloud as taught in\n",
      "Part III of this book.\n",
      "Technical Requirements : The technical requirements for the code and tutorials in this\n",
      "book are as follows:\n",
      "Python version 2 or 3 installed. This book was developed using Python version 2.7.11.\n",
      "SciPy and NumPy installed. This book was developed with SciPy version 0.17.0 and\n",
      "NumPy version 1.11.0.\n",
      "Matplotlib installed. This book was developed with Matplotlib version 1.5.1.\n",
      "Pandas installed. This book was developed with Pandas version 0.18.0.\n",
      "scikit-learn installed. This book was developed with scikit-learn 0.18.1.\n",
      "You do not need to match the version exactly, but if you are having problems running a\n",
      "speci\fc code example, please ensure that you update to the same or higher version as the library\n",
      "speci\fed.\n",
      "1.2.2 Machine Learning\n",
      "You do not need to be a machine learning expert, but it would be helpful if you knew how to\n",
      "navigate a small machine learning problem using scikit-learn. Basic concepts like cross-validation\n",
      "y. There are resources to in tutorials are described, but only brie\n",
      "go into these topics in more detail at the end of the book, but some knowledge of these areas\n",
      "might make things easier for you.\n",
      "1.2.3 Gradient Boosting\n",
      "You do not need to know the math and theory of gradient boosting algorithms, but it would be\n",
      "helpful to have some basic idea of the \feld. You will get a crash course in gradient boosting\n",
      "terminology and models, but we will not go into much technical detail. Again, there will be\n",
      "resources for more information at the end of the book, but it might be helpful if you can start\n",
      "with some idea about the technical details of the technique.\n",
      "1.3 Your Outcomes From Reading This Book\n",
      "This book will lead you from being a developer who is interested in XGBoost with Python to\n",
      "a developer who has the resources and capabilities to work through a new dataset end-to-end\n",
      "using Python and develop accurate gradient boosted models. Speci\fcally, you will know:1.4. What This Book is Not 5\n",
      "How to prepare data in scikit-learn for gradient boosting.\n",
      "How to evaluate and visualize gradient boosted models.\n",
      "How to save and load trained gradient boosted models.\n",
      "How to visualize, evaluate the importance of input variables.\n",
      "How to con\fgure and tune hyperparameters of gradient boosted models.\n",
      "There are a few ways you can read this book. You can dip into the tutorials as your need\n",
      "or interests motivate you. Alternatively, you can work through the book end-to-end and take\n",
      "advantage of how the tutorials build in complexity and range. I recommend the latter approach.\n",
      "To get the very most from this book, I recommend taking each tutorials and build upon\n",
      "them. Attempt to improve the results, apply the method to a similar but di\u000berent problem, and\n",
      "so on. Write up what you tried or learned and share it on your blog, social media or send me an\n",
      "email at jason@MachineLearningMastery.com . This book is really what you make of it and\n",
      "by putting in a little extra, you can quickly become a true force in applied gradient boosting.\n",
      "1.4 What This Book is Not\n",
      "This book solves a speci\fc problem of getting you, a developer, up to speed applying XGBoost\n",
      "to your own machine learning projects in Python. As such, this book was not intended to be\n",
      "everything to everyone and it is very important to calibrate your expectations. Speci\fcally:\n",
      "This is not a gradient boosting textbook . We will not cover the basic theory of\n",
      "how algorithms and related techniques work. There will be no equations. You are also\n",
      "expected to have some familiarity with machine learning basics, or be able to dive deeper\n",
      "into the theory yourself, if needed.\n",
      "This is not a Python programming book . We will not be spending a lot of time on\n",
      "Python syntax and programming (e.g. basic programming tasks in Python). You are\n",
      "expected to already be familiar with Python or a developer who can pick up a new C-like\n",
      "language relatively quickly.\n",
      "You can still get a lot out of this book if you are weak in one or two of these areas, but you\n",
      "may struggle picking up the language or require some more explanation of the techniques. If\n",
      "this is the case, see the Getting More Help chapter at the end of the book and seek out a good\n",
      "companion reference text.\n",
      "1.5 Summary\n",
      "It is a special time right now. The tools for fast gradient boosting have never been so good and\n",
      "XGBoost is the top of the stack. The pace of change with applied machine learning feels like it\n",
      "has never been so fast, spurred by the amazing results that the methods are showing in such a\n",
      "broad range of \felds. This is the start of your journey into using XGBoost and I am excited for\n",
      "you. Take your time, have fun and I'm so excited to see where you can take this amazing new\n",
      "technology.1.5. Summary 6\n",
      "Next in Part II, you will get a gentle introduction to the gradient boosting algorithm as\n",
      "described by primary sources. This will lay the foundation for understanding and using the\n",
      "XGBoost library in Python.Part II\n",
      "XGBoost Basics\n",
      "7Chapter 2\n",
      "A Gentle Introduction to Gradient\n",
      "Boosting\n",
      "Gradient boosting is one of the most powerful techniques for building predictive models. In this\n",
      "tutorial you will discover the gradient boosting machine learning algorithm and get a gentle\n",
      "introduction into where it came from and how it works. After reading this tutorial, you will\n",
      "know:\n",
      "The origin of boosting from learning theory and AdaBoost.\n",
      "How gradient boosting works including the loss function, weak learners and the additive\n",
      "model.\n",
      "How to improve performance over the base algorithm with various regularization schemes.\n",
      "Let's get started.\n",
      "2.1 Origin of Boosting\n",
      "The idea of boosting came out of the idea of whether a weak learner can be modi\fed to become\n",
      "better. Michael Kearns articulated the goal as the Hypothesis Boosting Problem stating the\n",
      "goal from a practical standpoint as:\n",
      "... an e\u000ecient algorithm for converting relatively poor hypotheses into very good\n",
      "hypotheses\n",
      "|Thoughts on Hypothesis Boosting , 1988.\n",
      "A weak hypothesis or weak learner is de\fned as one whose performance is at least slightly\n",
      "better than random chance. These ideas built upon Leslie Valiant's work on distribution free or\n",
      "Probability Approximately Correct (PAC) learning, a framework for investigating the complexity\n",
      "of machine learning problems. Hypothesis boosting was the idea of \fltering observations, leaving\n",
      "those observations that the weak learner can handle and focusing on developing new weak learns\n",
      "to handle the remaining di\u000ecult observations.\n",
      "82.2. AdaBoost the First Boosting Algorithm 9\n",
      "The idea is to use the weak learning method several times to get a succession of\n",
      "hypotheses, each one refocused on the examples that the previous ones found di\u000ecult\n",
      "and misclassi\fed. [...] Note, however, it is not obvious at all how this can be done\n",
      "|Probably Approximately Correct , page 152, 2013.\n",
      "2.2 AdaBoost the First Boosting Algorithm\n",
      "The \frst realization of boosting that saw great success in application was Adaptive Boosting or\n",
      "AdaBoost for short.\n",
      "Boosting refers to this general problem of producing a very accurate prediction rule\n",
      "by combining rough and moderately inaccurate rules-of-thumb.\n",
      "|A decision-theoretic generalization of on-line learning and an application to boosting , 1995.\n",
      "The weak learners in AdaBoost are decision trees with a single split, called decision stumps\n",
      "for their shortness. AdaBoost works by weighting the observations, putting more weight on\n",
      "di\u000ecult to classify instances and less on those already handled well. New weak learners are\n",
      "added sequentially that focus their training on the more di\u000ecult patterns.\n",
      "This means that samples that are di\u000ecult to classify receive increasing larger weights\n",
      "until the algorithm identi\fes a model that correctly classi\fes these samples\n",
      "|Applied Predictive Modeling , 2013.\n",
      "Predictions are made by majority vote of the weak learners' predictions, weighted by their\n",
      "individual accuracy. The most successful form of the AdaBoost algorithm was for binary\n",
      "classi\fcation problems and was called AdaBoost.M1.\n",
      "2.3 Generalization of AdaBoost as Gradient Boosting\n",
      "AdaBoost and related algorithms were recast in a statistical framework \frst by Breiman calling\n",
      "them ARCing algorithms.\n",
      "Arcing is an acronym for Adaptive Reweighting and Combining. Each step in an\n",
      "arcing algorithm consists of a weighted minimization followed by a recomputation of\n",
      "[the classi\fers] and [weighted input].\n",
      "|Prediction Games and Arching Algorithms , 1997.\n",
      "This framework was further developed by Friedman and called Gradient Boosting Machines.\n",
      "Later called just gradient boosting or gradient tree boosting. The statistical framework cast\n",
      "boosting as a numerical optimization problem where the objective is to minimize the loss of the\n",
      "model by adding weak learners using a gradient descent like procedure. This class of algorithms\n",
      "were described as a stage-wise additive model. This is because one new weak learner is added\n",
      "at a time and existing weak learners in the model are frozen and left unchanged.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_path = \"data/XGBoost with Python Gradient Boosted Trees with XGBoost and scikit-learn (Jason Brownlee) (z-lib.org).pdf\"\n",
    "start_page = 10\n",
    "end_page = 15\n",
    "\n",
    "with open(pdf_path, \"rb\") as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_number in range(start_page - 1, end_page):\n",
    "        page = pdf_reader.pages[page_number]\n",
    "        text += page.extract_text()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2374.75"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max_length+ len(text))/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running individual chain\n",
    "def run_chain(llm, prompt_candidate, text):\n",
    "    chain_chatgpt = LLMChain(llm=llm, prompt=prompt_candidate)\n",
    "    output = chain_chatgpt.run(text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bunch of templates for the prompt engineering experiments to create a list of facts from an article\n",
    "templates = [\"\"\"Create a list with all the facts contained in this article: {article}\"\"\",\n",
    "             \"\"\"Create a list with all the facts contained in this article: {article}. Let's think step by step.\"\"\",\n",
    "             \"\"\"Let's break down the information in this article, {article}, and compile a comprehensive list of facts.\"\"\",\n",
    "             \"\"\"We should systematically analyze {article} and create a detailed list of all the factual points.\"\"\",\n",
    "             ]\n",
    "\n",
    "prompt_candidates = [PromptTemplate(\n",
    "    input_variables=[\"article\"],\n",
    "    template=template,\n",
    ") for template in templates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['article'], output_parser=None, partial_variables={}, template='Create a list with all the facts contained in this article: {article}', template_format='f-string', validate_template=True),\n",
       " PromptTemplate(input_variables=['article'], output_parser=None, partial_variables={}, template=\"Create a list with all the facts contained in this article: {article}. Let's think step by step.\", template_format='f-string', validate_template=True),\n",
       " PromptTemplate(input_variables=['article'], output_parser=None, partial_variables={}, template=\"Let's break down the information in this article, {article}, and compile a comprehensive list of facts.\", template_format='f-string', validate_template=True),\n",
       " PromptTemplate(input_variables=['article'], output_parser=None, partial_variables={}, template='We should systematically analyze {article} and create a detailed list of all the factual points.', template_format='f-string', validate_template=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(template.template) for template in prompt_candidates])\n",
    "llm = OpenAI(temperature=0.0,max_tokens=4097-(max_length+ len(text))/4)\n",
    "# Save it to a csv table with the following columns: text_input, prompt candidates, output, model, approach (like zero shot or few shot etc...)\n",
    "for prompt_candidate in prompt_candidates:\n",
    "    output = run_chain(llm, prompt_candidate, text)\n",
    "    save_output(text, prompt_candidate.template, output, \"ChatGPT-3.5-Turbo\", \"Zero-Shot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_facts = PromptTemplate(input_variables=[\"article\"],template=\"Create a list with all the facts contained in this article: {article}. Let's think step by step.\")\n",
    "prompt_template_flashcards = PromptTemplate(input_variables=[\"facts_list\"], template=\"Create an flashcard for each of the following factual bullet points: {facts_list}. Let's think step by step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now run the entire thing as a sequence of chains with LangChain\n",
    "chain_facts = LLMChain(llm=llm, prompt=prompt_template_facts)\n",
    "chain_flashcards = LLMChain(llm=llm, prompt=prompt_template_flashcards)\n",
    "overall_chain = SimpleSequentialChain(chains=[chain_facts, chain_flashcards], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "10 Chapter 2. A Gentle Introduction to Gradient Boosting\n",
      "2.4 Loss Function\n",
      "The loss function is minimized by gradient descent. The loss function is de\fned as the\n",
      "difference between the predicted value and the actual value.\n",
      "Loss = Predicted Value - Actual Value\n",
      "The loss function is minimized by adding weak learners to the model that reduce the loss.\n",
      "The loss function is de\fned as the sum of the squared error of the model.\n",
      "Loss = (Predicted Value - Actual Value)2\n",
      "2.5 Weak Learners\n",
      "The weak learners used in gradient boosting are decision trees. Decision trees are used\n",
      "because they are simple to understand, easy to use and fast to train.\n",
      "The decision trees are grown one at a time and existing trees in the model are not changed.\n",
      "The decision trees are grown as deep as possible and the leaf nodes of the decision tree\n",
      "model the residuals, or the di\u000berence between the predicted value and the actual value.\n",
      "2.6 Additive Model\n",
      "The weak learners are added to the model one at a time. This is an additive model because\n",
      "the weak learners are added together to make a \fnal prediction.\n",
      "The \fnal prediction is made by summing the predictions of all the weak learners in the\n",
      "model.\n",
      "Final Prediction = Weak Learner1 Prediction + Weak Learner2 Prediction + ...\n",
      "The \fnal prediction is made by summing the predictions of all the weak learners in the\n",
      "model.\n",
      "2.7 Regularization\n",
      "Regularization is used to improve the performance of the model by penalizing models that\n",
      "are more complex.\n",
      "Regularization is used to reduce over\ftting by adding a penalty term to the loss function.\n",
      "The penalty term is a parameter that is multiplied by the complexity of the model.\n",
      "Loss = (Predicted Value - Actual Value)2 + Penalty Term * Model Complexity\n",
      "The penalty term is used to reduce the complexity of the model by limiting the number of\n",
      "weak learners that can be added to the model.\n",
      "Regularization can also be used to reduce the complexity of the model by limiting the\n",
      "size of the weak learners. This is done by limiting the depth of the decision trees.\n",
      "This is done by limiting the depth of the decision trees.\n",
      "11Summary\n",
      "In this tutorial you discovered the gradient boosting machine learning algorithm.\n",
      "You learned:\n",
      "The origin of boosting from learning theory and AdaBoost.\n",
      "How gradient boosting works including the loss function, weak learners and the additive\n",
      "model.\n",
      "How to improve performance over the base algorithm with various regularization schemes.\n",
      "This is the start of your journey into using XGBoost and I am excited for you. Take your\n",
      "time, have fun and I'm so excited to see where you can take this amazing new technology.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Flashcard 1: \n",
      "Question: What is the loss function used in gradient boosting?\n",
      "Answer: The loss function is defined as the difference between the predicted value and the actual value. Loss = Predicted Value - Actual Value.\n",
      "\n",
      "Flashcard 2: \n",
      "Question: What are the weak learners used in gradient boosting?\n",
      "Answer: The weak learners used in gradient boosting are decision trees. Decision trees are used because they are simple to understand, easy to use and fast to train.\n",
      "\n",
      "Flashcard 3: \n",
      "Question: How is the final prediction made in gradient boosting?\n",
      "Answer: The final prediction is made by summing the predictions of all the weak learners in the model. Final Prediction = Weak Learner1 Prediction + Weak Learner2 Prediction + ...\n",
      "\n",
      "Flashcard 4: \n",
      "Question: What is regularization used for in gradient boosting?\n",
      "Answer: Regularization is used to improve the performance of the model by penalizing models that are more complex. Regularization is used to reduce overfitting by adding a penalty term to the loss function. The penalty term is a parameter that is multiplied by the complexity of the model.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nFlashcard 1: \\nQuestion: What is the loss function used in gradient boosting?\\nAnswer: The loss function is defined as the difference between the predicted value and the actual value. Loss = Predicted Value - Actual Value.\\n\\nFlashcard 2: \\nQuestion: What are the weak learners used in gradient boosting?\\nAnswer: The weak learners used in gradient boosting are decision trees. Decision trees are used because they are simple to understand, easy to use and fast to train.\\n\\nFlashcard 3: \\nQuestion: How is the final prediction made in gradient boosting?\\nAnswer: The final prediction is made by summing the predictions of all the weak learners in the model. Final Prediction = Weak Learner1 Prediction + Weak Learner2 Prediction + ...\\n\\nFlashcard 4: \\nQuestion: What is regularization used for in gradient boosting?\\nAnswer: Regularization is used to improve the performance of the model by penalizing models that are more complex. Regularization is used to reduce overfitting by adding a penalty term to the loss function. The penalty term is a parameter that is multiplied by the complexity of the model.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the chain\n",
    "anki_flashcards = overall_chain.run(text)\n",
    "anki_flashcards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
